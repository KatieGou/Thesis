training command:
python training.py --max_grad_norm 5.0 --gradient_accumulation_steps 1 --use_img_layernorm 1 --output_dir output --add_od_labels --bert_model bert --model_name_or_path bert-base-multilingual-cased --warmup_steps 0 --do_train --train_batch_size 8 --ckpt_period 100 --max_iters 15000 --log_period 20 --data_dir data --dataset_file settings.yaml

visualization in tensorbord:
tensorboard --logdir=dir
open another terminal and run:
ssh -R  80:localhost:6006 localhost.run
from: http://localhost.run/docs/

modification of packages: 
Go to ~/.conda/envs/thesis/lib/python3.7/site-packages

1. modeling_bert.py: added model and config
BERT_PRETRAINED_MODEL_ARCHIVE_MAP={
    'KB/bert-base-swedish-cased': "https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/pytorch_model.bin"
}
BERT_PRETRAINED_CONFIG_ARCHIVE_MAP={
    'KB/bert-base-swedish-cased': "https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/config.json"
}
2. tokenization_bert.py: added vocab and position_embedding size
{'KB/bert-base-swedish-cased': "https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/vocab.txt"}
{'KB/bert-base-swedish-cased': 512}

relation.py:
train_captions.pt: {img_id: caption}

relation training command:
python relation.py --data_source coco --model_name_or_path output/bert-base-multilingual-cased_od_labels/checkpoint-0015000 --do_train --num_train_epochs 2 --save_steps 100 --evaluate_during_training

relation testing command:
python relation.py --do_test --data_source coco --output_examples --eval_model_dir relation_output/KB_bert-base-swedish-cased_od_labels/checkpoint-0-3000

lstm training command:
python lstm_training.py --max_grad_norm 5.0 --gradient_accumulation_steps 1 --use_img_layernorm 1 --output_dir output --add_od_labels --model_name_or_path KB/bert-base-swedish-cased --warmup_steps 0 --do_train --train_batch_size 8 --ckpt_period 100 --max_iters 15000 --log_period 20 --data_dir data --dataset_file settings.yaml

lstm relation training command:
python lstm_relation.py --data_source coco --model_name_or_path output/lstm_bert-base-multilingual-cased/checkpoint-0015000 --do_train --num_train_epochs 1 --save_steps 100 --evaluate_during_training

lstm relation testing command:
python lstm_relation.py --do_test --data_source coco --eval_model_dir relation_output/lstm_bert-base-multilingual-cased/checkpoint-0-2000

---------

baseline: text: input_ids (max_seq_length, )+ img_feature: (max_img_seq_len, 2054)
make max_seq_length=max_img_seq_len, concatnate img_feature to input_ids

baseline training command:
python baseline.py --img_feature_type mask_rcnn --model_name_or_path output/KB_bert-base-swedish-cased_mask_rcnn/checkpoint-0000500 --do_train --evaluate_during_training
python baseline.py --model_name_or_path output/KB_bert-base-swedish-cased_faster_rcnn/checkpoint-0000500 --do_train --evaluate_during_training --num_train_epochs 10

baseline testing command:
python baseline.py --img_feature_type faster_rcnn --do_test --eval_model_dir baseline_output/bert-base-swedish-cased/checkpoint-29-11500


Research question
How Bert can be extended and trained to perform VL tasks in Swedish?

Record precision and recall for each task!!!

embeddings from bert:
https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/

fine tuning: no object tags!!!
tensorborad to record!