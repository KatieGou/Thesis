training command:
python -m torch.distributed.launch --nproc_per_node=1 training.py --use_b 1 --max_grad_norm 5.0 --gradient_accumulation_steps 1 --use_img_layernorm 1 --output_dir output/ --bert_model bert --model_name_or_path KB/bert-base-swedish-cased --learning_rate 5e-05  --warmup_steps 0 --do_train --on_memory --train_batch_size 16 --ckpt_period 200 --max_iters 20000 --log_period 50 --data_dir data --dataset_file settings.yaml

modification of packages: 
1. modeling_bert.py: added model and config
BERT_PRETRAINED_MODEL_ARCHIVE_MAP={
    'KB/bert-base-swedish-cased': "https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/pytorch_model.bin"
}
BERT_PRETRAINED_CONFIG_ARCHIVE_MAP={
    'KB/bert-base-swedish-cased': "https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/config.json"
}
2. modeling_utils.py: print json objects
# 3. modeling_utils.py: print model load
4. modeling_utils.py: print init

relation.py:
train_captions.pt: {img_id: caption}

relation training command:
python relation.py --model_name_or_path output/checkpoint-0020000 --do_train --num_train_epochs 5 --max_seq_length 50 --do_lower_case --save_steps 200 --evaluate_during_training

relation testing command:
python relation.py --do_test --eval_model_dir relation_output/checkpoint-0-400 --do_lower_case

training and relation at the same time

modify:
early stopping
inference

need to cancel: args.num_labels, 
need to study: img_layer_norm_eps, use_img_layernorm, gradient_accumulation_steps, scheduler, adam, max_grad_norm, warmup_steps

baseline: text: input_ids (max_seq_length, )+ img_feature: (max_img_seq_len, 2054)
make max_seq_length=max_img_seq_len, concatnate img_feature to input_ids

research question: identify the most performing model (2*2), explore the effect of object tags

baseline training command:
python baseline.py --model_name_or_path output/checkpoint-0020000 --do_train --do_lower_case 